---
layout: post
title: "Speaking Faster (by a million times) Changes the World"
---

If we can speak faster (roughly a million times faster), we can tell people who have never tasted chocolate what chocolate tastes like, we won't need education or specialization, and we'll live in a much better world. 

Human communication is tricky. It can lead to misunderstanding, or even conflict. And countless number of literature works explore the theme. Some think it's beautiful that human communications isn't so clear or black and white, but I think it's merely a deficiency because we speak too slowly.

Human speaks so slowly that if your internet speed that slow, it would take 247 days to load a single photo on instagram [^photo-calc]. It is obviously unacceptable, but why do we not feel it being prohibitively slow? Because we are very good at choosing what is essential to convey and how to say them most effectively, which in computer terms, a extremely high compression ratio. And also because we are used to it. The world could change a lot if we could speak faster (through brain implant for example).

# How Human Compress Information Extremely Effectively

You are probably familiar with compression. From zipping flies to make them smaller, making a picture smaller to fit the upload file size constrains, to choosing lower quality Spotify music to save data, you already experienced how compression can make files smaller by getting rid of details and more efficiently encoding data. 

Usually, for video and audio, we can achieve 100:1 compression ratio (meaning the uncompressed data is 100 times bigger than the compressed one) with a some noticeable loss in quality. But human compression take it to an extreme, effortlessly skyrocketing it to 100,000:1, a thousand times higher than computer compression, because we can easily describe a picture with 400 words [^paragraph-calc]. To view it differently, the information in a books can easily fit into a photo. "A picture is worth a thousand words" is perhaps an understatement. 

The high compression ratio makes up for the slow speed. But how exactly do humans achieve that? They do by using labels, shared knowledge (oh no IB TOK flashbacks), context and [Gricean Maxims](https://www.youtube.com/watch?v=IJEaMtNN_dM). 

Compression algorithms often use something called a dictionary. Much like real dictionaries, it contains "words" and "meanings" [^dict]. Data that often repeats (e.g. the word "the") are replaced with something shorter (e.g. "t") while its replacement recorded in the dictionary. 

Human uses a dictionary on a much larger scale with labeling. While, to express an airplane in picture, the picture has to contain information on the exact shape of the wings, fuselage, etc., but humans can just use the word "airplane" to label it. To add more details, we would construct more elaborate sentences and paragraph to add labels and state their relationships. Labeling are more prominent in technical fields like natural sciences. For example, "prime number" would be a very short label for its long definition [^definition]. Labels like "3d orbitals" would capture more information about atomic structure. Our extensive use of labels and their lengthy and complicated definition makes education necessary (among other reasons which we'll explore later). 

Unlike in computer compression, where the dictionary is sent along with data, the dictionary we use is not communicated in a conversation because our communication speed is too slow for that (recall the last time you tried to talk about a topic unfamiliar to your listener and you had to explain everything). The dictionary is assumed to be shared through life experience and education. The shared knowledge also includes social expectations, cultural conventions, etc. In that sense, misunderstanding and cultural shock are all results of the dictionary between you and the other people being mismatched. The problems sometimes take a lot of time to resolve because communication is slow compared to the body of shared knowledge.

Naturally, since communicating information is time consuming, humans have unspoken rules which purpose is to increase commutation efficiency called [Gricean Maxims](https://www.youtube.com/watch?v=IJEaMtNN_dM) ([original paper](https://www.ucl.ac.uk/ls/studypacks/Grice-Logic.pdf)). These are (paraphrased):
1. Maxim of Quantity: Give no more or less information than necessary.
2. Maxim of Quality: Give information that is cooperative.
3. Maxim of Relation: Give information that is related. 
4. Maxim of Manner: Be clear. 

Using these principles, we can reasonably infer information not explicitly stated. For example, if someone answers you "there's a garage down the road" to your statement "I am out of petrol", you can infer that "garage" probably is a british slag for a gas station. 

Chocolate is impossible to describe to someone who hasn't tasted it because the word chocolate doesn't have corresponding meaning in their dictionary and taste (and many other sensory information) contains too much information to be communicated through words. Technologies that we developed, such as screens and speakers can already, through their high speed, allow people to see and hear completely novel things. Even though, much often, the source of the information is from the natural world (such as photograph and recording), it can also be from a human such as painting and animation, which are painfully made with hours and hours of work, sometimes with nothing but painfully typed text like early-day computer graphics. Essentially, information is slowly outputted by a human, stored using technologies, and outputted quickly through our faster senses like eyes and ears.

# Consequences of Slow Communication Speed 

1. Education becomes necessary for a well developed society
2. Specialization improves efficiency

Everyone understands the importance of education (ignoring those who believe education is a big conspiracy to condition citizens into obedience). From the lens of human communication, education is essentially a decade long process of loading the huge dictionary of human shared knowledge into a person. It takes so long because absorbing information is very slow. 

Information is slow to absorb, but can travel quickly in one's brain. Looking for information in the world is painfully slow, especially without aid (like Google). But looking for information you already know is very fast. You've probably had moments when you remember someone you've read but can't find where it's from. Similarly, writing about a familiar subject is so much easier than an unfamiliar because you can look up information in your brain. As a society we have decided that loading all the information upfront through decade long education is more efficient than learning on demand. 

Specialization is also a product of the slow communication speed. Collaboration is inherently limited because we cannot get ideas across fast enough for very complex tasks. That is why books are usually written with one person, math theorems are rarely team efforts. But the same time, dividing tasks allows one to devote more time to excel at the task. So naturally, tasks are divided at intersections without much information flow. Physicists wouldn't need to know much about history and vice versa. 

# Predictions for Machine-Brain Interface

Imagine that ono day technologies is advanced enough that we can develop a chip that can be implanted in everyone's brain that gives everyone a million times faster communication speed. How different would the world be? It will be very different, most significantly, education and specialization would no long exist. 

Education would be abolished because when you can read a book in one second, there is no need to read it in advance. You would just read a book or learn a skill when you need it (like how they can "load" a skill in The Matrix). More over, it's even possible that there is no need to learn or "load" anything. The act of loading in a computer essentially copies data from a slower medium to a faster medium (e.g. loading from hard drive to RAM), so does learning (e.g. moving information from books to your brain). As technologies get better, searching information through the internet might get faster than searching it in your head. For example, when you need a quote for an essay, you wouldn't read 10 books to find it (even though it just takes 10 second), instead you would tell Google what you need and select from the thousands of individual quotes. To you subjectively, it would feel like the content is "already there" and you just need to use it. 

We see the trend of going from loading everything upfront to loading as needed in the development of computer. We used to spend hours download programs or days buying CDs of programs for computer to run, to using more and more applications run in browsers (called web apps). When you open Google Docs, it opens very quickly even though word processing applications can take minutes to hour to download, because it loads only the component you need now, moving some computation to the cloud, leaving the functions you don't need right now to be loaded right you need it. Streaming movies is better than downloading movies. It's the same for purchasing goods. The wisdom goes only buys the things you need when you needed. Generally, it is a much more efficient strategy to learn along the way than to learning upfront [^memorization].

Specialization would also be eliminated. Collaboration could truly be made effective, even for complex and indivisible tasks. Because learning would not be necessary, there would no need to specialize in order to devote more time to excel at a narrow task. More over, specialization is does not fundamentally increase productivity or creativity, it merely compensates for our slow communication speed. Generalization is natural direction for development. If all else equals, we would much prefer a computer (which is a general computational machine) than a calculator [^raspi]. But in the 1950s, when computer hardware was very expensive, there were specialized computers like the the IBM 1401 that specialize in reading cards, copying tapes, and printing output. The research into general AI also highlights the importance and desirability of having a general intelligence. Theses trends illustrate that specialization only occurs as a necessity when technologies is not good enough for generalization.

More speculatively, perhaps the world would have a single consciousness as tightly connect brains function as one giant brain? Maybe there would be no more conflict of interest that leads to war and suffering because experiences can be easily shared? Or would independent thinking be diminished to the point we all become sheeple controlled by large supplier of knowledge like Google? We don't yet know, but let's hope for the better.

[^photo-calc]: I used a photo posed by my friend on instagram, which, with the heavy compression down by instagram is 142KB (kilobytes, or 1.3 megabits). The speech speed used is 130 words per minute ([source](https://wordcounter.net/blog/2016/06/02/101702_how-fast-average-person-speaks.html)). Assuming one word is on average 5 letters, and one letter takes 5 bits to encode (because we only need to encode 26 lower case letters), that makes the speech of our speech 54 bits/second. 1.3 megabits ÷ 54 bits/second = 21307037 seconds = 247 days. 

[^paragraph-calc]: Same as the previous photo. 400 words × 5 letters/word × 5 bits/letter = 10000 bits. 

[^dict]: For the programmer out there, I am referring to keys and values. 

[^definition]: The astute reader might notice I glossed over the problem with definition. What would you define something with but more labels? The popular definition of "prime number" is "a number that cannot be divided by 1 and itself", but "number", "divided" are both labels that need their definition. If you (like me) have wondered about this and tried to see dictionary solve this, you will find definitions are often circular. For a simple exercise, start the word "love". Fortunately, you and I are not the first one to be troubled by the definition problem. Mathematics (especially formalism) deals with accurately defining structures extensively by essentially establishing fundamental "words" (called strings) that do not have definitions of their own but are rather "defined" by their relations to other strings by rules of string manipulation. For a more through (and fascinating) discussion on the topic, I recommend the book Godel, Escher, Bach by Douglas R. Hofstadter. 

[^memorization]: There are serval more technical examples of this: caching dynamically as data is being read or changed (memorization technique) is usually less buggy and more efficient than precomputing a lookup table (such as [noria](https://github.com/mit-pdos/noria) and APFS). Just-in-time compiling or interpreting is gaining popularity over compiling. 

[^raspi]: If the costs are equal, you would rather buy a raspberry pi than a micro-controller or even a specially designed circuit. 

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" />